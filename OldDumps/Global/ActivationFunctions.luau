local ActivationFunctions = {}
-- Generated By GPT

-- Sigmoid
ActivationFunctions.Sigmoid = function(x, derivative)
	if derivative then
		local fx = 1 / (1 + math.exp(-x))
		return fx * (1 - fx)
	else
		return 1 / (1 + math.exp(-x))
	end
end

-- Tanh
ActivationFunctions.Tanh = function(x, derivative)
	if derivative then
		local fx = math.tanh(x)
		return 1 - fx * fx
	else
		return math.tanh(x)
	end
end

-- ReLU
ActivationFunctions.ReLU = function(x, derivative)
	if derivative then
		return x > 0 and 1 or 0
	else
		return math.max(0, x)
	end
end

-- Leaky ReLU
ActivationFunctions.LeakyReLU = function(x, derivative)
	local alpha = 0.01
	if derivative then
		return x > 0 and 1 or alpha
	else
		return x > 0 and x or alpha * x
	end
end

-- ELU
ActivationFunctions.ELU = function(x, derivative)
	local alpha = 1.0
	if derivative then
		return x >= 0 and 1 or alpha * math.exp(x)
	else
		return x >= 0 and x or alpha * (math.exp(x) - 1)
	end
end

-- Swish (x * sigmoid(x))
ActivationFunctions.Swish = function(x, derivative)
	local sigmoid = 1 / (1 + math.exp(-x))
	if derivative then
		return sigmoid + x * sigmoid * (1 - sigmoid)
	else
		return x * sigmoid
	end
end

-- Identity
ActivationFunctions.Identity = function(x, derivative)
	if derivative then
		return 1
	else
		return x
	end
end

-- Softplus
ActivationFunctions.Softplus = function(x, derivative)
	if derivative then
		return 1 / (1 + math.exp(-x))
	else
		return math.log(1 + math.exp(x))
	end
end

return ActivationFunctions
